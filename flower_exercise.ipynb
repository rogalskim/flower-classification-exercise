{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Make Imports and Set Up Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copy\n",
    "from os import listdir, makedirs\n",
    "from os.path import isdir, join, splitext\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CUDA GPUs available. Using GeForce RTX 2080 SUPER with CUDA 7.5 capability.\n"
     ]
    }
   ],
   "source": [
    "def setup_device() -> torch.device:\n",
    "    if (not torch.cuda.is_available()):\n",
    "        print(\"No CUDA GPUs found. CPU selected as training device.\")\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "    device_id = 0\n",
    "    device = torch.device(f\"cuda:{device_id}\")\n",
    "    count = torch.cuda.device_count()\n",
    "    name = torch.cuda.get_device_name(device_id)\n",
    "    capability = torch.cuda.get_device_capability(device_id)\n",
    "    print(f\"{count} CUDA GPUs available. Using {name} with CUDA {capability[0]}.{capability[1]} capability.\")\n",
    "    return device\n",
    "\n",
    "\n",
    "device = setup_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Divide the Dataset into Categories\n",
    "The used dataset doesn't contain any explicit category labels. However, the source images are sorted by category, with each of the 17 categories having exactly 80 images. Therefore we can label the images by simply counting them. The code assumes the raw images are placed into _data/jpg_ directory. It splits the data into training, validation and testing subsets, then creates a subdirectory for each subset in _data_ dir. In each of those, another set of subdirs is created -- one for each category, named after the category index (0 through 16) and flower images from that category are copied inside. The number of copied files for each subset is defined in the _subset_splits_ dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_jpg(file_path: str) -> bool:\n",
    "    path_root, extension = splitext(file_path)\n",
    "    return extension.lower() == \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_0001.jpg',\n",
       " 'image_0002.jpg',\n",
       " 'image_0003.jpg',\n",
       " 'image_0004.jpg',\n",
       " 'image_0005.jpg',\n",
       " 'image_0006.jpg',\n",
       " 'image_0007.jpg',\n",
       " 'image_0008.jpg',\n",
       " 'image_0009.jpg',\n",
       " 'image_0010.jpg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_path = \"data/jpg\"\n",
    "category_count = 17\n",
    "images_per_category = 80\n",
    "\n",
    "# How many images in each category should fall into a data subset\n",
    "subset_splits = {\"training\": 56, \"validation\": 16, \"testing\": 8}\n",
    "assert sum(subset_splits.values()) == images_per_category\n",
    "\n",
    "image_list = [file for file in listdir(raw_data_path) if is_file_jpg(file)]\n",
    "assert len(image_list) == category_count * images_per_category\n",
    "image_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_images_to_categories(category_count: int, images_per_category: int, images: list) -> dict:\n",
    "    categories = {}\n",
    "    for category_index in range(category_count):\n",
    "        first_image_in_category = category_index * images_per_category\n",
    "        last_image_in_category = first_image_in_category + images_per_category\n",
    "        categories[category_index] = images[first_image_in_category:last_image_in_category]\n",
    "    return categories\n",
    "        \n",
    "\n",
    "category_dict = assign_images_to_categories(category_count, images_per_category, image_list)\n",
    "\n",
    "assert len(category_dict.keys()) == category_count\n",
    "assert len(category_dict[category_count - 1]) == images_per_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_subsets(category_dict: dict, subset_splits: dict) -> (dict, dict, dict):\n",
    "    train, valid, test = {}, {}, {}\n",
    "    first_validation_image = subset_splits[\"training\"]\n",
    "    first_testing_image = first_validation_image + subset_splits[\"validation\"]\n",
    "    \n",
    "    for cat_index, cat_images  in category_dict.items():\n",
    "        train[cat_index] = cat_images[:first_validation_image]\n",
    "        valid[cat_index] = cat_images[first_validation_image:first_testing_image]\n",
    "        test[cat_index] = cat_images[first_testing_image:]\n",
    "    \n",
    "    return train, valid, test\n",
    "    \n",
    "    \n",
    "training_images, validation_images, testing_images = split_data_into_subsets(category_dict, subset_splits)\n",
    "\n",
    "assert len(training_images.keys()) == len(validation_images.keys()) == len(testing_images.keys()) == category_count\n",
    "assert len(training_images[10]) == subset_splits[\"training\"]\n",
    "assert len(validation_images[4]) == subset_splits[\"validation\"]\n",
    "assert len(testing_images[16]) == subset_splits[\"testing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path: str) -> None:\n",
    "    if not isdir(path):\n",
    "        makedirs(path)\n",
    "        \n",
    "\n",
    "def create_subset_data_directories(subset_dict: dict, subset_name: str, raw_data_path: str) -> None:\n",
    "    subset_dir_path = join(\"data\", subset_name)\n",
    "    create_directory(subset_dir_path)\n",
    "    \n",
    "    for category_index, category_images in subset_dict.items():\n",
    "        category_path = join(subset_dir_path, str(category_index))\n",
    "        create_directory(category_path)\n",
    "        \n",
    "        for image in category_images:\n",
    "            source_path = join(raw_data_path, image)\n",
    "            destination = join(category_path, image)\n",
    "            copy(source_path, destination)\n",
    "        \n",
    "\n",
    "create_subset_data_directories(training_images, \"training\", raw_data_path)\n",
    "create_subset_data_directories(validation_images, \"validation\", raw_data_path)\n",
    "create_subset_data_directories(testing_images, \"testing\", raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-process Data and Load into Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The network that will be used in transfer learning has been pre-trained using normalized data. Therefore the same\n",
    "transformation must be performed for new data, for the training to be effective. Below are the values used for original\n",
    "normalization.\n",
    "'''\n",
    "normalization_means = [0.485, 0.456, 0.406]\n",
    "normalization_stds = [0.229, 0.224, 0.225]\n",
    "final_image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These transformations should help the network to learn translation, rotation and size invariance, to reduce over-training. \n",
    "Additionally they normalize the input data to make it more statistically similar to the data that the network was \n",
    "pre-trained on.\n",
    "'''\n",
    "training_transforms = [tr.RandomRotation(degrees=10, expand=True),\n",
    "                       tr.RandomResizedCrop(size=final_image_size, scale=[0.75, 1.0]),\n",
    "                       tr.ToTensor(),\n",
    "                       tr.Normalize(mean=normalization_means, std=normalization_stds)]\n",
    "\n",
    "testing_transforms = [tr.Resize(size=final_image_size + 8),\n",
    "                      tr.CenterCrop(size=final_image_size),\n",
    "                      tr.ToTensor(),\n",
    "                      tr.Normalize(mean=normalization_means, std=normalization_stds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "def make_data_loader(data_path: str, transforms: list, batch_size: int) -> DataLoader:\n",
    "    image_transformation = tr.Compose(transforms)\n",
    "    data_set = ImageFolder(root=data_path, transform=image_transformation)\n",
    "    should_pin_memory = torch.cuda.is_available()\n",
    "    loader = DataLoader(dataset=data_set, shuffle=True, pin_memory=should_pin_memory, batch_size=batch_size)\n",
    "    return loader\n",
    "    \n",
    "    \n",
    "training_loader = make_data_loader(\"data/training\", training_transforms, batch_size)\n",
    "validation_loader = make_data_loader(\"data/validation\", training_transforms, batch_size)\n",
    "testing_loader = make_data_loader(\"data/testing\", testing_transforms, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "According to comparisons (such as this one https://learnopencv.com/wp-content/uploads/2019/06/Pre-Trained-Model-Comparison.png),\n",
    "ResNet50 seems to offer good balance between training time and accuracy.\n",
    "'''\n",
    "pre_trained_model = torchvision.models.resnet50(pretrained=True)\n",
    "pre_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_feature_count = pre_trained_model.fc.in_features\n",
    "pre_trained_feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This custom classifier will be used to replace the original ResNet's classifier layers. This way, we can adapt the model \n",
    "to classify our 17 flower categories using its pre-trained feature extraction layers. It will have three layers, as this\n",
    "is often sufficient for good accuracy.\n",
    "'''\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_features: int, \n",
    "                 hidden1_size: int, \n",
    "                 hidden2_size: int, \n",
    "                 output_categories: int,\n",
    "                 dropout: float) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(input_features, hidden1_size)\n",
    "        self.hidden2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.output = nn.Linear(hidden2_size, output_categories)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout(nn.functional.relu(self.hidden1(input_tensor)))\n",
    "        x = self.dropout(nn.functional.relu(self.hidden2(x)))\n",
    "        raw_category_scores = self.output(x)\n",
    "        return raw_category_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_feature_detector_training(network: nn.Module) -> None:\n",
    "    for parameter in network.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "        \n",
    "def prepare_network_for_transfer_learning(network: nn.Module) -> nn.Module:\n",
    "    disable_feature_detector_training(network)\n",
    "    custom_classifier = CustomClassifier(pre_trained_feature_count, 1024, 512, category_count, dropout=0.05)\n",
    "    network.fc = custom_classifier\n",
    "    network = network.to(device)\n",
    "    return network\n",
    "    \n",
    "    \n",
    "network = prepare_network_for_transfer_learning(pre_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_step(network: nn.Module, training_loader: DataLoader, device: torch.device, optimizer, criterion) -> float:\n",
    "    network.train()\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for images, labels in training_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        raw_output = network.forward(images)\n",
    "        loss = criterion(input=raw_output, target=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "    avg_loss /= len(training_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(raw_output: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    class_probabilities = nn.functional.softmax(raw_output, dim=1)\n",
    "    predicted_classes = torch.topk(input=class_probabilities, k=1, dim=1)[1]\n",
    "    resized_labels = labels.view(predicted_classes.shape[0], -1)\n",
    "    prediction_matches = predicted_classes == resized_labels\n",
    "    batch_average_accuracy = torch.mean(prediction_matches.type(torch.FloatTensor))\n",
    "    return batch_average_accuracy\n",
    "\n",
    "\n",
    "def run_evaluation_step(network: nn.Module, loader: DataLoader, device: torch.device, criterion) -> (float, float):\n",
    "    network.eval()\n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            raw_output = network.forward(images)\n",
    "            loss = criterion(input=raw_output, target=labels)\n",
    "            accuracy = calculate_accuracy(raw_output, labels)\n",
    "            \n",
    "            avg_loss += loss\n",
    "            avg_accuracy += accuracy\n",
    "    \n",
    "    avg_loss /= len(loader)\n",
    "    avg_accuracy /= len(loader)\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network: nn.Module, training_loader: DataLoader, validation_loader: DataLoader, device: torch.device) -> None:\n",
    "    \n",
    "    # Hyperparameters\n",
    "    learn_rate = 0.0003\n",
    "    optimizer = optim.Adam(params=network.fc.parameters(), lr=learn_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    epoch_count = 10\n",
    "    \n",
    "    for epoch in range(epoch_count):\n",
    "        print(f\"Epoch {epoch+1} / {epoch_count}\")\n",
    "        train_loss = run_training_step(network, training_loader, device, optimizer, criterion)\n",
    "        print(f\"Training Loss: {train_loss}\")\n",
    "        valid_loss, valid_accuracy =  run_evaluation_step(network, validation_loader, device, criterion)\n",
    "        print(f\"Validation Loss: {valid_loss}\")\n",
    "        print(f\"Validation Accuracy: {valid_accuracy * 100:.2f}%\\n\")\n",
    "        \n",
    "    test_loss, test_accuracy =  run_evaluation_step(network, testing_loader, device, criterion)\n",
    "    print(\"\\nTest Results\")\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n",
      "Training Loss: 2.488348197937012\n",
      "Validation Loss: 2.0703892707824707\n",
      "Validation Accuracy: 55.94%\n",
      "\n",
      "Epoch 2 / 10\n",
      "Training Loss: 1.4780928532282511\n",
      "Validation Loss: 1.2181166410446167\n",
      "Validation Accuracy: 72.50%\n",
      "\n",
      "Epoch 3 / 10\n",
      "Training Loss: 0.7229420105616252\n",
      "Validation Loss: 0.790314793586731\n",
      "Validation Accuracy: 81.88%\n",
      "\n",
      "Epoch 4 / 10\n",
      "Training Loss: 0.4000472366809845\n",
      "Validation Loss: 0.576329231262207\n",
      "Validation Accuracy: 86.56%\n",
      "\n",
      "Epoch 5 / 10\n",
      "Training Loss: 0.2668887515862783\n",
      "Validation Loss: 0.5161110758781433\n",
      "Validation Accuracy: 85.94%\n",
      "\n",
      "Epoch 6 / 10\n",
      "Training Loss: 0.17539071142673493\n",
      "Validation Loss: 0.5010616183280945\n",
      "Validation Accuracy: 84.69%\n",
      "\n",
      "Epoch 7 / 10\n",
      "Training Loss: 0.17492026885350545\n",
      "Validation Loss: 0.48568567633628845\n",
      "Validation Accuracy: 85.00%\n",
      "\n",
      "Epoch 8 / 10\n",
      "Training Loss: 0.12750202069679897\n",
      "Validation Loss: 0.4384729564189911\n",
      "Validation Accuracy: 87.50%\n",
      "\n",
      "Epoch 9 / 10\n",
      "Training Loss: 0.09021254802743593\n",
      "Validation Loss: 0.40239444375038147\n",
      "Validation Accuracy: 88.12%\n",
      "\n",
      "Epoch 10 / 10\n",
      "Training Loss: 0.11092286209265391\n",
      "Validation Loss: 0.3419328033924103\n",
      "Validation Accuracy: 90.00%\n",
      "\n",
      "\n",
      "Test Results\n",
      "Test Loss: 0.4599529206752777\n",
      "Test Accuracy: 84.38%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_network(network, training_loader, validation_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
