{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Make Imports and Set Up Device\n",
    "All imports are put into this first cell to make managing them easier. Next we detect if a compatible GPU is available in the local machine. If so, it will be used to train our network, which is much faster than using a CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copy\n",
    "from os import listdir, makedirs\n",
    "from os.path import isdir, join, splitext\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 CUDA GPUs available. Using GeForce RTX 2080 SUPER with CUDA 7.5 capability.\n"
     ]
    }
   ],
   "source": [
    "def setup_device() -> torch.device:\n",
    "    if (not torch.cuda.is_available()):\n",
    "        print(\"No CUDA GPUs found. CPU selected as training device.\")\n",
    "        return torch.device(\"cpu\")\n",
    "    \n",
    "    device_id = 0\n",
    "    device = torch.device(f\"cuda:{device_id}\")\n",
    "    count = torch.cuda.device_count()\n",
    "    name = torch.cuda.get_device_name(device_id)\n",
    "    capability = torch.cuda.get_device_capability(device_id)\n",
    "    print(f\"{count} CUDA GPUs available. Using {name} with CUDA {capability[0]}.{capability[1]} capability.\")\n",
    "    return device\n",
    "\n",
    "\n",
    "device = setup_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Divide the Dataset into Categories\n",
    "The used dataset doesn't contain any explicit category labels. However, the source images are sorted by category, with each of the 17 categories having exactly 80 images. Therefore we can label the images by simply counting them. The code assumes the raw images are placed into _data/jpg_ directory. It splits the data into training, validation and testing subsets, then creates a subdirectory for each subset in _data_ dir. In each of those, another set of subdirs is created - one for each category, named after the category index (0 through 16) and flower images from that category are then copied inside. The number of copied files for each subset is defined in the _subset\\_splits_ dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_jpg(file_path: str) -> bool:\n",
    "    path_root, extension = splitext(file_path)\n",
    "    return extension.lower() == \".jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_0001.jpg',\n",
       " 'image_0002.jpg',\n",
       " 'image_0003.jpg',\n",
       " 'image_0004.jpg',\n",
       " 'image_0005.jpg',\n",
       " 'image_0006.jpg',\n",
       " 'image_0007.jpg',\n",
       " 'image_0008.jpg',\n",
       " 'image_0009.jpg',\n",
       " 'image_0010.jpg']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_path = \"data/jpg\"\n",
    "category_count = 17\n",
    "images_per_category = 80\n",
    "\n",
    "# How many images in each category should fall into a data subset\n",
    "subset_splits = {\"training\": 56, \"validation\": 16, \"testing\": 8}\n",
    "assert sum(subset_splits.values()) == images_per_category\n",
    "\n",
    "image_list = [file for file in listdir(raw_data_path) if is_file_jpg(file)]\n",
    "assert len(image_list) == category_count * images_per_category\n",
    "image_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_images_to_categories(category_count: int, images_per_category: int, images: list) -> dict:\n",
    "    categories = {}\n",
    "    for category_index in range(category_count):\n",
    "        first_image_in_category = category_index * images_per_category\n",
    "        last_image_in_category = first_image_in_category + images_per_category\n",
    "        categories[category_index] = images[first_image_in_category:last_image_in_category]\n",
    "    return categories\n",
    "        \n",
    "\n",
    "category_dict = assign_images_to_categories(category_count, images_per_category, image_list)\n",
    "\n",
    "assert len(category_dict.keys()) == category_count\n",
    "assert len(category_dict[category_count - 1]) == images_per_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_subsets(category_dict: dict, subset_splits: dict) -> (dict, dict, dict):\n",
    "    train, valid, test = {}, {}, {}\n",
    "    first_validation_image = subset_splits[\"training\"]\n",
    "    first_testing_image = first_validation_image + subset_splits[\"validation\"]\n",
    "    \n",
    "    for cat_index, cat_images  in category_dict.items():\n",
    "        train[cat_index] = cat_images[:first_validation_image]\n",
    "        valid[cat_index] = cat_images[first_validation_image:first_testing_image]\n",
    "        test[cat_index] = cat_images[first_testing_image:]\n",
    "    \n",
    "    return train, valid, test\n",
    "    \n",
    "    \n",
    "training_images, validation_images, testing_images = split_data_into_subsets(category_dict, subset_splits)\n",
    "\n",
    "assert len(training_images.keys()) == len(validation_images.keys()) == len(testing_images.keys()) == category_count\n",
    "assert len(training_images[10]) == subset_splits[\"training\"]\n",
    "assert len(validation_images[4]) == subset_splits[\"validation\"]\n",
    "assert len(testing_images[16]) == subset_splits[\"testing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path: str) -> None:\n",
    "    if not isdir(path):\n",
    "        makedirs(path)\n",
    "        \n",
    "\n",
    "def create_subset_data_directories(subset_dict: dict, subset_name: str, raw_data_path: str) -> None:\n",
    "    subset_dir_path = join(\"data\", subset_name)\n",
    "    create_directory(subset_dir_path)\n",
    "    \n",
    "    for category_index, category_images in subset_dict.items():\n",
    "        category_path = join(subset_dir_path, str(category_index))\n",
    "        create_directory(category_path)\n",
    "        \n",
    "        for image in category_images:\n",
    "            source_path = join(raw_data_path, image)\n",
    "            destination = join(category_path, image)\n",
    "            copy(source_path, destination)\n",
    "        \n",
    "\n",
    "create_subset_data_directories(training_images, \"training\", raw_data_path)\n",
    "create_subset_data_directories(validation_images, \"validation\", raw_data_path)\n",
    "create_subset_data_directories(testing_images, \"testing\", raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-process Data and Load into Data Loaders\n",
    "Before it can be used for training, the data needs to be pre-processed and assigned to DataLoaders, which will be used to provide image batches to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The network that will be used in transfer learning has been pre-trained using normalized data. Therefore the same\n",
    "transformation must be performed for the new data, for the training to be effective. Below are the values used for original\n",
    "normalization, taken from Torchvision documentation.\n",
    "'''\n",
    "normalization_means = [0.485, 0.456, 0.406]\n",
    "normalization_stds = [0.229, 0.224, 0.225]\n",
    "final_image_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "These transformations should help the network to learn translation, rotation and size invariance. In turn, this should \n",
    "improve its ability to generalize and reduce over-training. Additionally, we normalize the input data to make it more \n",
    "statistically similar to the data that the base network was pre-trained on.\n",
    "'''\n",
    "training_transforms = [tr.RandomRotation(degrees=10, expand=True),\n",
    "                       tr.RandomResizedCrop(size=final_image_size, scale=[0.75, 1.0]),\n",
    "                       tr.ToTensor(),\n",
    "                       tr.Normalize(mean=normalization_means, std=normalization_stds)]\n",
    "\n",
    "testing_transforms = [tr.Resize(size=final_image_size + 8),\n",
    "                      tr.CenterCrop(size=final_image_size),\n",
    "                      tr.ToTensor(),\n",
    "                      tr.Normalize(mean=normalization_means, std=normalization_stds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "def make_data_loader(data_path: str, transforms: list, batch_size: int) -> DataLoader:\n",
    "    image_transformation = tr.Compose(transforms)\n",
    "    data_set = ImageFolder(root=data_path, transform=image_transformation)\n",
    "    should_pin_memory = torch.cuda.is_available()\n",
    "    loader = DataLoader(dataset=data_set, shuffle=True, pin_memory=should_pin_memory, batch_size=batch_size)\n",
    "    return loader\n",
    "    \n",
    "    \n",
    "training_loader = make_data_loader(\"data/training\", training_transforms, batch_size)\n",
    "validation_loader = make_data_loader(\"data/validation\", training_transforms, batch_size)\n",
    "testing_loader = make_data_loader(\"data/testing\", testing_transforms, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the Network\n",
    "According to comparisons (such as https://learnopencv.com/wp-content/uploads/2019/06/Pre-Trained-Model-Comparison.png),\n",
    "ResNet50 is a powerful image classification model that offers good balance between training time and accuracy. Torchvision offers models like ResNet50 already pre-trained on the ImageNet dataset. This dataset contains flowers among its many categories. This means that the pre-trained ResNet already knows how to recognize flower features. To use it in our problem we just need to replace its classification layers (the _fc_ module), so that they classify images as the categories we are interested in. After a short fine-tune training, this new model should be able to correctly classify our 17 flower categories.  This technique is called transfer learning, as it \"transfers\" the knowledge acquired in one problem domain into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_model = torchvision.models.resnet50(pretrained=True)\n",
    "pre_trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_feature_count = pre_trained_model.fc.in_features\n",
    "pre_trained_feature_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This custom classifier will be used to replace the original ResNet's classifier layers. This way, we can adapt the model \n",
    "to classify our 17 flower categories, while still using its pre-trained feature extraction layers. The classifier will have\n",
    "three fully-connected layers, as this number is often sufficient for good accuracy.\n",
    "'''\n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_features: int, \n",
    "                 hidden1_size: int, \n",
    "                 hidden2_size: int, \n",
    "                 output_categories: int,\n",
    "                 dropout: float) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(input_features, hidden1_size)\n",
    "        self.hidden2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.output = nn.Linear(hidden2_size, output_categories)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.dropout(nn.functional.relu(self.hidden1(input_tensor)))\n",
    "        x = self.dropout(nn.functional.relu(self.hidden2(x)))\n",
    "        raw_category_scores = self.output(x)\n",
    "        return raw_category_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For this particular problem we disable training of ResNet's feature detection layers altogether. Since they already should\n",
    "be able to handle flower images, there is little point in training them further, and we can save a lot of compute by\n",
    "freezing these layers. We will only train the new fully-connected classification layers. Layer sizes for the classifier\n",
    "are chosen more or less arbitrarily, based on what worked in previous similar tasks. \n",
    "'''\n",
    "\n",
    "def disable_feature_detector_training(network: nn.Module) -> None:\n",
    "    for parameter in network.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "        \n",
    "def prepare_network_for_transfer_learning(network: nn.Module) -> nn.Module:\n",
    "    disable_feature_detector_training(network)\n",
    "    custom_classifier = CustomClassifier(pre_trained_feature_count, 1024, 512, category_count, dropout=0.05)\n",
    "    network.fc = custom_classifier\n",
    "    network = network.to(device)\n",
    "    return network\n",
    "    \n",
    "    \n",
    "network = prepare_network_for_transfer_learning(pre_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the Network\n",
    "The network is now ready to be trained. Since this is just a simple proof of concept, we will not use much logging of the process, nor perform a hyperparameter search. Hyperparameter values are based on solutions for similar tasks. For simplicity, we will also not save the model or make checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_step(network: nn.Module, training_loader: DataLoader, device: torch.device, optimizer, criterion) -> float:\n",
    "    network.train()\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for images, labels in training_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        raw_output = network.forward(images)\n",
    "        loss = criterion(input=raw_output, target=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "    avg_loss /= len(training_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(raw_output: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    class_probabilities = nn.functional.softmax(raw_output, dim=1)\n",
    "    predicted_classes = torch.topk(input=class_probabilities, k=1, dim=1)[1]\n",
    "    resized_labels = labels.view(predicted_classes.shape[0], -1)\n",
    "    prediction_matches = predicted_classes == resized_labels\n",
    "    batch_average_accuracy = torch.mean(prediction_matches.type(torch.FloatTensor))\n",
    "    return batch_average_accuracy\n",
    "\n",
    "\n",
    "def run_evaluation_step(network: nn.Module, loader: DataLoader, device: torch.device, criterion) -> (float, float):\n",
    "    network.eval()\n",
    "    avg_loss = 0\n",
    "    avg_accuracy = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            raw_output = network.forward(images)\n",
    "            loss = criterion(input=raw_output, target=labels)\n",
    "            accuracy = calculate_accuracy(raw_output, labels)\n",
    "            \n",
    "            avg_loss += loss\n",
    "            avg_accuracy += accuracy\n",
    "    \n",
    "    avg_loss /= len(loader)\n",
    "    avg_accuracy /= len(loader)\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network: nn.Module, training_loader: DataLoader, validation_loader: DataLoader, device: torch.device) -> None:\n",
    "    \n",
    "    # Hyperparameters based on solutions for similar tasks; Adam is quite fast, good for prototyping; often needs low learning rate. \n",
    "    # Criterion based on problem type (multiclass classification).\n",
    "    learn_rate = 0.0003\n",
    "    optimizer = optim.Adam(params=network.fc.parameters(), lr=learn_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    epoch_count = 10\n",
    "    \n",
    "    start_time = time()\n",
    "    for epoch in range(epoch_count):\n",
    "        print(f\"Epoch {epoch+1} / {epoch_count}\")\n",
    "        train_loss = run_training_step(network, training_loader, device, optimizer, criterion)\n",
    "        print(f\"Training loss: {train_loss}\")\n",
    "        valid_loss, valid_accuracy =  run_evaluation_step(network, validation_loader, device, criterion)\n",
    "        print(f\"Validation loss: {valid_loss}\")\n",
    "        print(f\"Validation accuracy: {valid_accuracy * 100:.2f}%\\n\")\n",
    "    print(f\"Total training duration: {(time() - start_time)/60:.2f} minutes\")\n",
    "        \n",
    "    start_time = time()\n",
    "    test_loss, test_accuracy =  run_evaluation_step(network, testing_loader, device, criterion)\n",
    "    print(\"\\nTest Results\")\n",
    "    print(f\"Test loss: {test_loss}\")\n",
    "    print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n",
    "    print(f\"Test duration: {(time() - start_time)/60:.2f} minutes\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n",
      "Training loss: 2.578791093826294\n",
      "Validation loss: 2.198519229888916\n",
      "Validation accuracy: 50.31%\n",
      "\n",
      "Epoch 2 / 10\n",
      "Training loss: 1.629003357887268\n",
      "Validation loss: 1.326111912727356\n",
      "Validation accuracy: 73.12%\n",
      "\n",
      "Epoch 3 / 10\n",
      "Training loss: 0.8108087658882142\n",
      "Validation loss: 0.902256190776825\n",
      "Validation accuracy: 76.56%\n",
      "\n",
      "Epoch 4 / 10\n",
      "Training loss: 0.4491083880265554\n",
      "Validation loss: 0.6707755327224731\n",
      "Validation accuracy: 79.06%\n",
      "\n",
      "Epoch 5 / 10\n",
      "Training loss: 0.34807762801647185\n",
      "Validation loss: 0.475945383310318\n",
      "Validation accuracy: 89.69%\n",
      "\n",
      "Epoch 6 / 10\n",
      "Training loss: 0.2302742898464203\n",
      "Validation loss: 0.4471200108528137\n",
      "Validation accuracy: 86.56%\n",
      "\n",
      "Epoch 7 / 10\n",
      "Training loss: 0.1794422114888827\n",
      "Validation loss: 0.4181794822216034\n",
      "Validation accuracy: 85.94%\n",
      "\n",
      "Epoch 8 / 10\n",
      "Training loss: 0.1523560826977094\n",
      "Validation loss: 0.3603057563304901\n",
      "Validation accuracy: 90.31%\n",
      "\n",
      "Epoch 9 / 10\n",
      "Training loss: 0.11256239066521327\n",
      "Validation loss: 0.36138972640037537\n",
      "Validation accuracy: 89.38%\n",
      "\n",
      "Epoch 10 / 10\n",
      "Training loss: 0.09355117653807005\n",
      "Validation loss: 0.31333470344543457\n",
      "Validation accuracy: 90.00%\n",
      "\n",
      "Total training duration: 1.71 minutes\n",
      "\n",
      "Test Results\n",
      "Test loss: 0.6249529123306274\n",
      "Test accuracy: 80.21%\n",
      "Test duration: 0.02 minutes\n"
     ]
    }
   ],
   "source": [
    "train_network(network, training_loader, validation_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Performance Assessment\n",
    "The performance of the final model is satisfactory. After only around 2 minutes of training on a consumer-grade PC, it reaches validation top-1 accuracy of nearly 90%, which should be considered very good. The inference on the test set (136 images not seen during training) takes only around 1 second and results in ~80% average top-1 accuracy. The test accuracy is markedly lower than that achieved for the validation set, but is still decent. The inference speed (lets assume ~10ms per image) is excellent, considering this is a completely un-optimized proof-of-concept model and there was no hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Answers to Task Questions\n",
    "#### How would you share your findings with the client?\n",
    "\n",
    "I would prepare a short presentation describing the network’s performance in the context of the proposed application. Key use cases would be identified and analyzed, to see if the performance would be acceptable in these scenarios. I would also like to make a small live demo application using the prototype, such as a simple script running classification on some input images (e.g. manually collected from the Web) showing the model’s speed and accuracy. \n",
    "\n",
    "#### What would your comments be to a colleague building the app, regarding the model?\n",
    "\n",
    "I would describe the model’s computational and memory requirements for training and inference, as well as the properties of expected input and output data. We could discuss options for improving model computational performance or accuracy and other possible changes in its implementation.\n",
    "\n",
    "#### The amount of time you have spent on this task.\n",
    "\n",
    "I’ve spend roughly an entire day writing the code and documentation. A lot of time was consumed by implementing a clean solution to divide the data set into training, validation and test subsets. Some documentation reading was also necessary. All in all, I’d say it was about 8 hours of work time in total."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
